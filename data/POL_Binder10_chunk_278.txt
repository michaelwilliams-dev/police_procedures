system is appropriate with the sensitivity of the data it was controlled on as well as the controls intended to ensure the safety of data. 2.5 Where the AI system will be interacting with other systems, (be they internal or external), Developers and System Operators shall ensure that the permissions used by the system are only provided as required for functionality and are risk assessed. This includes ensuring identities used by the AI system are constrained in scope and privilege to the access required. This includes external AI and non-AI fail-safes if necessary. 2.6 If a System Operator chooses to work with an external model provider, they shall undertake a due diligence assessment of that provider√ïs security. This assessment could involve implement scanning and isolation/sandboxing when importing third-party models, serialised weights or untrusted third-party code. 2.7 If a Developer and/or System Operator decides to use an external library, they shall complete a due diligence assessment.[footnote 25] This assessment could consider whether the model can be obtained as a safe model and if not, then doing checks to ensure the library has controls that prevent the system loading untrusted models without immediately exposing itself to arbitrary code execution. Principle 3: Model the threats to your system[footnote 26] Primarily applies to: Developers and System Operators [OWASP 2024, WEF 2024, Nvidia 2023, ENISA 2023, Google 2023, G7 2023, NCSC 2023, Deloitte 2023] 3.1 Developers and System Operators shall undertake modelling of the threats to a system as part of their risk management process. NCSC Guidelines for Secure AI System Development: This modelling could include understanding the potential impacts to all AI-responsible stakeholders, end-users, and wider society if an AI component becomes compromised or behaves unexpectedly. Additionally, the modelling could be informed by AI-specific attacks and failure modes, as well as more traditional IT system attacks. The modelling could factor the total range of possible outputs, (including worst case scenarios), from AI components and their impact on the system. 3.1.1 The risk management process shall be conducted to address any security risks that arise when a new setting or configuration option is implemented at any stage of the AI lifecycle. 3.1.2 As part of this process, Developers shall create a document that includes a list of adversarial motivations and possible attack routes in line with those motivations. The type of attacks could include indirect attacks where attackers poison data which might later be used by, or sent to, the model. 3.1.3 Developers shall manage the risks associated with models that provide multiple functionality, where increased functionality leads to increased risk. For example, where a multi-modal model is being used but only single modality is used for system function. 3.2 Data controllers should conduct a data protection impact assessment when necessary as a measure under UK data protection obligations to determine what controls are needed. 3.3 Where threats are identified that cannot be resolved by Developers, this shall be communicated to System Operators and End-users to allow them to appropriately threat model their systems. 3.4