online, consistent with the rate of doubling deepfakes every six months.3 AI-generated misinformation is already impacting elections. Just two days before Slovakiaâs recent parliamentary vote, social media was flooded with a fake audio clip of a prominent party leader allegedly discussing plans to buy votes.4 This incident resulted in a significant loss for the implicated party, spotlighting the risks of AI-powered misinformation.4 The problem is borderless: in January 2024, during New Hampshireâs presidential primary season, a deepfake audio of President Joe Biden, which falsely claimed that primaries did not matter was widely circulated. The risk is clear: the next generation of voters may eventually lose faith in not only what we read and see but in the political system itself. About 63% of Americans believe that altered or fabricated videos and images cause significant confusion around the facts of current events and issues.6 At a time when political polarization is high and civic engagement is low, the rapid development of AI-driven disinformation threatens to overwhelm our ability to discern what is real, eroding the very essence of our democracy.7 Policy Idea To combat misinformation and increase trust in the electoral process, an AI-Generated Content Disclosure Law is necessary. This law would require all media platformsâsocial media sites, news outlets, and streaming servicesâto label content created or significantly modified by AI. The label must be visible and state that the content was generated or altered using artificial intelligence (defined as advanced AI systems capable of generating or altering content that misleads audiences). Enforcement would involve private companies implementing detection tools internally, under government oversight, to monitor and flag content. This ensures consumers can engage with online content more transparently and confidently. Policy Analysis The ease with which these technologies can now create hyper- realistic content creates a massive political liability.8 This aforementioned AI-Generated Content Disclosure Law addresses the root of this problem by increasing transparency for the public when they engage with this media. Instead of just restricting this content, the policy focuses on educating viewers and building trust. When viewers know whether AI created something, they can engage more critically and responsibly with the media, as suggested by President Bidenâs executive order last fall.9 Additionally, there is existing precedent. For example, the European Unionâs Digital Services Act includes provisions for flagging disinformation, though it fails to specifically address AI-generated content specifically.10 Similarly, some platforms have voluntarily introduced their own labels for AI content, but these efforts are often inconsistent across media platforms.11 By mandating such a labeling initiative across platforms across the US, this law would create a more unified approach to resolving the issue. Enforcing this policy would require platforms to develop or obtain advanced detection tools for identifying AI-generated content, leading to increased administrative and operational costs.12 Additionally, funding for these efforts would need to come from both government and platforms, potentially with support from media literacy grants, which would be challenging. However, studies have shown that disinformation costs the global economy over $78 billion annually, and AI-generated