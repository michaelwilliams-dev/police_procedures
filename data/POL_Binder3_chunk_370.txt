would need to come from both government and platforms, potentially with support from media literacy grants, which would be challenging. However, studies have shown that disinformation costs the global economy over $78 billion annually, and AI-generated deepfakes pose growing threats to political stability and trust.3 Hence, by investing in detection and transparency tools, both platforms and governments can reduce these costs by contributing to a more informed and engaged electorate, mitigating the spread of harmful AI- driven misinformation. In the long run, these savings, combined with increased public trust, make this a worthwhile investment for safeguarding media integrity.13 Highlights  AI-generated misinformation poses a threat to the strength of democratic institutions.1  Any piece of media (i.e., images, videos, audio) produced or altered by AI must carry a label similar to ÒThis content was generated using artificial intelligence.Ó  Platforms would be responsible for identifying AI-generated content through AI detection tools, ensuring that the proper labels are applied.  If a platform fails to flag AI- generated content appropriately, penalties ranging from fines to content removal would be imposed by a regulatory agency, such as the Federal Communications Commission.  Ensures that consumers can easily identify AI-generated or AI-altered content, thereby reducing misinformation and enhancing the legitimacy of online information.  Implementing the AI- Generated Content Disclosure Law requires bipartisan support, public outreach, FCC incentives for compliance, and transparent success metrics like reduced flagged AI content, with penalties funding media literacy programs and input from a community advisory board to refine guidelines. Implementation Implementing the AI- Generated Content Disclosure Law would require coordinated legislative efforts and public outreach to ensure compliance and build public trust in political media. Platforms should be mandated to label all AI-created or altered media with clear disclaimers, ensuring users are informed about the nature of the content they interact and engage with online. However, since public trust is at an all-time low, platforms may initially resist compliance, citing increased costs and operational burdens.14,17 In response, the FCC should create incentive programs or grants for smaller platforms to adopt detection tools, reducing financial barriers to compliance.15 If additional funding becomes necessary, penalty revenue from non-compliant platforms and grants from public-private partnerships would help sustain the program. If funding falls short, additional measures like increased public- private partnerships, reallocation of FCC resources, or supplemental government appropriations would be necessary to sustain the program. Additionally, early-stage outreach to nonprofit media literacy organizations and advocacy groups would help raise awareness of the need for transparent labeling practices, preventing the companies from framing the policy as excessively burdensome. Considering resistance from tech companies and political lobbying, securing bipartisan support would be essential. Pre- committee support can be garnered by engaging bipartisan caucuses like the Problem Solvers caucus and hosting stakeholder roundtables with legislators, industry leaders, and public interest groups. Representative Cathy McMorris Rodgers, Chair of the House Committee on Energy and Commerce, could introduce the bill due to her prior leadership on tech issues, in alignment with existing federal efforts