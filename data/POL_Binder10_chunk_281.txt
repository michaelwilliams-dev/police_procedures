required. Particular attention should be given to the use of open-source models, where the responsibility of model maintenance and security becomes complex. 7.3 Developers and System Operators should be prepared to failover to alternate solutions for mission-critical systems, if their security criteria are not met.[footnote 34] 7.3.1 Where training data has been sourced from publicly available sources, Developers and Data controllers shall need to validate that such training data will not compromise the integrity of their security protocols. 7.3.2 Data controllers should continually monitor the source of publicly available data that could be used for creating a model, such as for changes in the data sources that may risk creating vulnerabilities. Principle 8: Document your data, models and prompts[footnote 35] Primarily applies to: Developers [OWASP 2024, WEF 2024, NCSC 2023, Cisco 2022, Microsoft 2022, ICO 2020] 8.1 Developers shall document and maintain a clear audit trail of their model design and post-deployment maintenance plans. 8.1.1 Developers should ensure that the document includes security-relevant information, such as the sources of training data (including fine-tuning data and human or other operational feedback), intended scope and limitations, guardrails, cryptographic hashes or signatures, retention time, suggested review frequency and potential failure modes. 8.1.2 Developers should pay particular attention to document areas of model and system complexity that could lead to unexpected security issues, including details of software dependencies and configurations. 8.2 Developers should ensure that model outputs include only the necessary information for downstream purposes and do not include additional meta-data that might be used for honing attacks against the model. Principle 9: Conduct appropriate testing and evaluation[footnote 36] Primarily applies to: Developers [OWASP 2024, WEF 2024, Nvidia 2023, NCSC 2023, ENISA 2023, Google 2023, G7 2023] 9.1 Developers shall ensure that no models, applications or systems are released that haven√ït been tested as part of a security assessment process. 9.2 Developers shall validate that AI models perform as intended through testing. 9.2.1 Developers shall work closely with System Operators for post-deployment testing when maintaining a system. (see 2.1.2 for more details) 9.2.2 Evaluations of AI systems should involve red teaming or other adversarial testing as part of a whole system approach. 9.2.3 Evaluations of AI systems should be undertaken by suitably skilled testers. Where possible, this should be an independent external evaluation. 9.1.1 9.3 Developers should perform benchmarking as part of their risk management process throughout the AI development lifecycle (see principle 2 for more detail). 9.4 Developers should ensure that the findings from the testing and evaluation are shared with System Operators, to inform their own testing and evaluation. Secure Deployment Principle 10: Communication and processes associated with end-users[footnote 37] Primarily applies to: Developers and System Operators 10.1 In the context of AI, Developers shall state clearly to end-users (where possible) which aspects of security the end-user is responsible for and are transparent about where and how their data might be used, accessed or stored (for example, if it is used for model retraining, or reviewed by employees or partners). 10.2 Developers