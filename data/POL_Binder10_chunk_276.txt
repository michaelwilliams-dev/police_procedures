who use AI in their own infrastructure which has been created by an external provider These organisations are a System Operator and therefore are in scope of relevant parts of the Code and the Software Resilience Code of Practice. Software vendors who create AI in-house and implement it within their infrastructure These organisations are a Developer and System Operator and therefore are in scope of this Code and the Software Resilience Code of Practice. Software vendors who only use third-party AI (components) for their in-house use These organisations are a System Operator and therefore are in scope of relevant parts of the Code and the Software Resilience Code of Practice. Organisation that creates an AI system for in-house use These organisations are a Developer and therefore are in scope of this Code. Organisation that only uses third-party AI components These organisations are a System Operator and therefore are in scope of relevant parts of the Code. AI Vendors Organisations that offer or sell models and components, but do not play a role in developing or deploying them, are not in scope of this Code. These organisations are in scope of the Software Code of Practice and Cyber Governance Code. What does the terminology mean in the voluntary Code of Practice? We have used ÒshallÓ and ÒshouldÓ terminology for each provision in the voluntary Code to align with the wording used by standards development organisations.[footnote 23] The table below sets out the definitions of these words in the context of the voluntary nature of this Code of Practice. Term Definition Shall Indicates a requirement for the voluntary Code Should Indicates a recommendation for the voluntary Code Can/could Indicates where something is possible, for example, that an organisation or individual is able to do something Code of Practice Principles Secure Design Principle 1: Raise staff awareness of threats and risks Primarily applies to: System Operators, Developers, and Data Controllers [NIST 2022, NIST 2023, ASD 2023, WEF 2024, OWASP 2024, MITRE 2024, Google 2023, ESLA 2023, Cisco 2022, Deloitte 2023, Microsoft 2022] 1.1. Organisations shall establish and maintain a continuous security awareness program to educate their staff about the evolving threat landscape specific to AI systems. 1.1.1. The AI-Security security awareness content shall be reviewed and updated where necessary at least every six months. 1.1.2. AI-specific security awareness training could be incorporated into existing infosec training for staff. 1.2. Developer organisations should provide their staff with regular updates on the latest security threats and vulnerabilities that could impact AI systems 1.2.1. These updates should be communicated through multiple channels, such as security bulletins, newsletters, or internal knowledge-sharing platforms, to ensure broad dissemination and understanding among the staff. 1.3. Developers shall receive training in secure coding techniques specific to AI development, with a focus on preventing and mitigating security vulnerabilities in AI algorithms, models, and associated software. 1.3.1 Developer training should also include content on how developers may leverage AI/LLMs to improve code security 1.4 Developers shall receive awareness training in the characteristics of