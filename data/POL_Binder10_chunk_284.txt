Street, Westminster, London, SW1A 2BQ. DSIT plans to arrange workshops with industry to help gather feedback. 4.3 Following the Call for Views, we will review the feedback provided. We plan to publish a response which provides an overview of the key themes from the Call for Views and DSITÕs future direction of travel. If there is support for a global technical standard, we will look to take this forward alongside increasing our participation across global standard development organisations. Annex A: Research findings Overview of research studies and objectives The research field of AI Security is still nascent and has developed for the last 5 years. DSIT has commissioned targeted research to establish an initial evidence base to inform the development of our policy interventions, notably the Code of Practice.[footnote 39] The specific studies involved: An assessment of the cyber security risks to AI (https://www.gov.uk/government/publications/research-on-the-cyber- security-of-ai/cyber-security-risks-to-artificial-intelligence) by Grant Thornton and Manchester Metropolitan University was completed in February 2024. A survey of 350 UK businesses to understand how org anisations are approaching AI (https://www.gov.uk/government/publications/research-on-the-cyber-security-of-ai/ai-cyber-security-survey-main-report), particularly regarding cyber security. The survey was conducted by IFF Research and ran from January to February 2024. A literature review mapping the technical and policy recommendations made by industry and other governments. This was completed in February 2024, conducted by Professor Peter Garraghan of Mindgard, using information published since 2020.[footnote 40] A literature review of research on the cyber security of AI (https://assets.publishing.service.gov.uk/media/663cf1b2bd01f5ed3279388e/Study_of_research_and_guidance_on_the_c yber_security_of_AI_-_Queens_University_Belfast_literature_review.pdf) involving an in-depth analysis of more than 415 publications, completed in February 2024 by QueenÕs University Belfast. The key findings from the research are outlined below. The vulnerabilities and threats across various AI technologies are broadly similar to each other. The exploitation of vulnerabilities in an AI system can have a substantial impact on end-users, such as the loss of sensitive data linked to consumers and employees as well as providing malicious actors with a way of breaching an organisationÕs infrastructure. Organisations generally lack awareness and understanding of what security should be built into models and systems and whether practices/processes should be in place when adopting AI to protect their organisations. Key organisations, governments and standards development organisations advocate for security requirements for AI models and systems. Vulnerabilities found in AI systems can enable the models and systems to be weaponised which can result in cyber attacks and significant harms on users.[footnote 41] The majority of the research conducted in the field of the Security of AI is being conducted by academic institutions. Out of the 415 sources on the cyber security of AI fully analysed by Queens University Belfast, only 28% were created by industry organisations. Threats to AI technologies The assessment of the cyber security risks to AI was commissioned to provide DSIT with an up-to-date analysis of the risk landscape of AI technologies. The assessment was formed from the findings of two literature reviews and subsequent interviews. The first literature review mapped and evaluated any previous research on the cyber security risks of AI, including known vulnerabilities. The second literature